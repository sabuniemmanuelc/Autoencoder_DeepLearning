{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOheoOW3CmGHbaHFNapEQLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabuniemmanuelc/Autoencoder_DeepLearning/blob/main/DRLPortfolioAllocationWithRiskMitigation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SPUqJaTFd8lF",
        "outputId": "e59abcf0-2324-4073-c18f-d24d5afa8aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  5 of 5 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 197ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot accumulate on a scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c7930601aeb>\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Train DRL model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mtrain_drl_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-0c7930601aeb>\u001b[0m in \u001b[0;36mtrain_drl_model\u001b[0;34m(env, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# Sample action from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrl_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;31m# Train the model using experience replay or other techniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Update the DRL model based on the observed reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-0c7930601aeb>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Implement risk management techniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# For example, apply constraint on maximum drawdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mmax_drawdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_capital\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_capital\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mmax_drawdown_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m  \u001b[0;31m# Example threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mviolation_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m  \u001b[0;31m# Penalty for violating constraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot accumulate on a scalar"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gym\n",
        "import yfinance as yf\n",
        "\n",
        "# Function to fetch data from Yahoo Finance\n",
        "def fetch_yahoo_finance_data(tickers, start_date, end_date):\n",
        "    data = yf.download(tickers, start=start_date, end=end_date)\n",
        "    return data['Adj Close']\n",
        "\n",
        "# Function to calculate returns and covariances\n",
        "def calculate_returns_and_covariances(data):\n",
        "    returns = data.pct_change().dropna()\n",
        "    covariances = returns.cov()\n",
        "    return returns.mean(), covariances\n",
        "\n",
        "# Define the environment for portfolio allocation\n",
        "class PortfolioAllocationEnv(gym.Env):\n",
        "    def __init__(self, initial_capital, risk_free_rate, asset_returns, asset_covariances):\n",
        "        super(PortfolioAllocationEnv, self).__init__()\n",
        "        self.initial_capital = initial_capital\n",
        "        self.current_capital = initial_capital\n",
        "        self.risk_free_rate = risk_free_rate\n",
        "        self.asset_returns = asset_returns\n",
        "        self.asset_covariances = asset_covariances\n",
        "        self.num_assets = len(asset_returns)\n",
        "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.num_assets,), dtype=np.float32)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(self.num_assets+1,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_capital = self.initial_capital\n",
        "        self.portfolio_weights = np.ones(self.num_assets) / self.num_assets\n",
        "        self.time_step = 0\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        return np.concatenate([self.portfolio_weights, [self.current_capital]])\n",
        "\n",
        "    def step(self, action):\n",
        "        # Update portfolio weights based on action\n",
        "        self.portfolio_weights = action / np.sum(action)\n",
        "\n",
        "        # Calculate portfolio return and update capital\n",
        "        portfolio_return = np.dot(self.portfolio_weights, self.asset_returns)\n",
        "        self.current_capital *= (1 + portfolio_return)\n",
        "\n",
        "        # Calculate risk-adjusted performance metric (e.g., Sharpe ratio)\n",
        "        sharpe_ratio = (portfolio_return - self.risk_free_rate) / np.std(self.asset_returns)\n",
        "\n",
        "        # Reward function: maximize Sharpe ratio\n",
        "        reward = sharpe_ratio\n",
        "\n",
        "        # Implement risk management techniques\n",
        "        # For example, apply constraint on maximum drawdown\n",
        "        max_drawdown = np.max(np.maximum.accumulate(self.current_capital) - self.current_capital)\n",
        "        max_drawdown_threshold = 0.2  # Example threshold\n",
        "        violation_penalty = -10  # Penalty for violating constraint\n",
        "        if max_drawdown > max_drawdown_threshold:\n",
        "            reward += violation_penalty\n",
        "            done = True  # End episode due to constraint violation\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        # Update time step\n",
        "        self.time_step += 1\n",
        "\n",
        "        # Return observation, reward, done, info\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "# Define the deep reinforcement learning model\n",
        "def build_drl_model(input_shape, num_actions):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    hidden_layer = Dense(64, activation='relu')(input_layer)\n",
        "    output_layer = Dense(num_actions, activation='softmax')(hidden_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Main training loop\n",
        "def train_drl_model(env, num_episodes, batch_size):\n",
        "    input_shape = env.observation_space.shape\n",
        "    num_actions = env.action_space.shape[0]\n",
        "    drl_model = build_drl_model(input_shape, num_actions)\n",
        "    drl_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Sample action from the model\n",
        "            action = drl_model.predict(np.array([state]))[0]\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # Train the model using experience replay or other techniques\n",
        "            # Update the DRL model based on the observed reward\n",
        "            # Implement exploration-exploitation strategy\n",
        "            state = next_state\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch data from Yahoo Finance\n",
        "    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']  # Example tickers\n",
        "    start_date = '2020-01-01'\n",
        "    end_date = '2022-01-01'\n",
        "    data = fetch_yahoo_finance_data(tickers, start_date, end_date)\n",
        "\n",
        "    # Calculate returns and covariances\n",
        "    asset_returns, covariances = calculate_returns_and_covariances(data)\n",
        "\n",
        "    # Define other parameters for the environment\n",
        "    initial_capital = 1000000\n",
        "    risk_free_rate = 0.02\n",
        "\n",
        "    # Create environment\n",
        "    env = PortfolioAllocationEnv(initial_capital, risk_free_rate, asset_returns, covariances)\n",
        "\n",
        "    # Define training parameters\n",
        "    num_episodes = 1000\n",
        "    batch_size = 32\n",
        "\n",
        "    # Train DRL model\n",
        "    train_drl_model(env, num_episodes, batch_size)\n"
      ]
    }
  ]
}